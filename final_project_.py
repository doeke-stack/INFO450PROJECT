# -*- coding: utf-8 -*-
"""Final Project .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19SmhTHAs5XS22MQnXlkKY413T4agRdxR
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

sns.set(style="whitegrid")

df = pd.read_csv("IndividualAssistanceHousingRegistrantsLargeDisasters (2).csv")

"""i start off by importing all my libraries and loading in the dataset"""

df.head()
df.info()
df.describe(include="all")

"""Based off of what I'm seeing key variables are currently missing, gross income is missing for some registants, and assessment reliated fields are missing for the majority of rows. Waterlevel is also completely empty

Within the data types certain columns have floats when they should not and need to be converted

there are also outliers that need to be fixed certain values are extremely high

based off of what we can see you can identify the columns that are important such as income equity, residence type fairness, geographic fairness as well
"""

# Drop useless columns since water level is empty
df = df.drop(columns=["waterLevel"])  # empty

# Convert dtypes
df["damagedZipCode"] = df["damagedZipCode"].astype("Int64").astype("string")
df["damagedStateAbbreviation"] = df["damagedStateAbbreviation"].astype("category")
df["residenceType"] = df["residenceType"].astype("category")

# Fix income outliers
df = df[df["grossIncome"] < 500000]  # optional cutoff

# Handle missing income
df["grossIncome"] = df["grossIncome"].fillna(df["grossIncome"].median())

"""Based off of the information returned I made edits to the code and cleaned it up


1.   dropped water level column since it was empty
2.   converted data types items such as zip codes should not be stored as numbers because they are identifiers not values
state abbreviation was turned into a category as well as residence type
3. there was a large income outlier in so only values under 500k were included because it is unlikely FEMA registants have incomes above 500k
4. for every missing income value it will be filled in with the median instead of average because it provides a realistic income

**Step 2 - create the TSA cross tab**
"""

# Step 2: TSA Eligibility Crosstab
tsa_crosstab = pd.crosstab(
    index=df["residenceType"],
    columns=df["damagedStateAbbreviation"],
    values=df["tsaEligible"],
    aggfunc="mean"
)

tsa_crosstab

"""This shows TSA eligibility by residence type what they are staying in by the state and territory. based on the info shared it shows there are several residence type in states that have a combination of nan values which could be because certain states have few applicants for specific resident types the only states that appeared to be meaningful seemed to be FL, LA, PR, TX, NC, and MC."""

avg_repairs_by_state = (
    df.groupby("damagedStateAbbreviation")["repairAmount"]
      .mean()
      .sort_values(ascending=False)
)

avg_repairs_by_state

"""The groupby analysis helped show that texas had the highest mean repair amount at 8,458. and then the states following in order. several states showed NAN values which measn there were no repair claims in the dataset but is not missing data. The pattern shows that disaster servity and geographic vulnerability influence repair payouts

**Bar Chart TSA eligibiliy rate by state/territory**
"""

import pandas as pd
import matplotlib.pyplot as plt

# Calculate TSA eligibility rate per state
tsa_rate = df.groupby("damagedStateAbbreviation")["tsaEligible"].mean().sort_values(ascending=False)

plt.figure(figsize=(12,6))
tsa_rate.plot(kind="bar")
plt.title("TSA Eligibility Rate by State/Territory")
plt.ylabel("Eligibility Rate (%)")
plt.xlabel("State/Territory")
plt.tight_layout()
plt.show()

"""Based off of the bar chart provided by eligibility rate in each state it is prevalent that territories such as the virgin islands and puerto rico have the highest eligibility rate however states with very small sample sizes can look more extreme. Each bar shows the percentage of customers eligible for tsa assistance

**Histogram - distribution of repair amount**
"""

plt.figure(figsize=(10,6))
plt.hist(df["repairAmount"].dropna(), bins=40)
plt.title("Distribution of Repair Amounts")
plt.xlabel("Repair Amount ($)")
plt.ylabel("Frequency")
plt.show()

"""This helps you see wether repaird amounts are clusterd, skewed, or contain outliars. the data is right skewed. Based off of the data it looks as though most repairs are very small amounts. This shows that most repairs are not extreme in payments and frequently is getting paid out at 5000.

**Box plot - Repair amount accross residence types**
"""

import seaborn as sns

plt.figure(figsize=(22,7))
sns.boxplot(data=df, x="residenceType", y="repairAmount")
plt.title("Repair Amount by Residence Type")
plt.xlabel("Residence Type")
plt.ylabel("Repair Amount ($)")
plt.show()

"""The graph shows that most of the repair amounts for specific resident types are similar to what the histogram shows which the repair amounts do not go above 10000 in the median range, we also can see that the highest outlier is the house/duplex range. The results help us get a better grapse of the repair amounts for each resident type but it may not be fully accurate because, depending on where the repairs are, and outliers can inflate the median.

**Analyst choice: How gross income affects TSA eligibility (bar chart) **
"""

# Create income bins
bins = [0, 25000, 50000, 75000, 100000, 150000, 500000]
labels = ["0–25k", "25–50k", "50–75k", "75–100k", "100–150k", "150k+"]

df["incomeBracket"] = pd.cut(df["grossIncome"], bins=bins, labels=labels, include_lowest=True)

income_tsa = df.groupby("incomeBracket")["tsaEligible"].mean().reset_index()
income_tsa["tsaEligible"] = income_tsa["tsaEligible"] * 100  # convert to %
income_tsa

"""The graph provided shows the probability of the created income brackets of recieving TSA and 0-25k income brackets have a 43 percent chance of receiving TSA benefits"""

plt.figure(figsize=(10,6))
sns.barplot(data=income_tsa, x="incomeBracket", y="tsaEligible")
plt.title("TSA Eligibility Rate by Household Income Level")
plt.xlabel("Gross Income Bracket")
plt.ylabel("TSA Eligibility Rate (%)")
plt.xticks(rotation=45)
plt.show()

"""This provides a more clear understanding of how likely and the correlation between different income brackets and the chances of them getting approved for TSA, as the income gets higher the eligibility rate gets lower and lower. This may suggest that TSA may be prioritizing lower income as they intend

**Part 2: Inferential Statistics (confidence intervals)**
"""

import numpy as np
import scipy.stats as stats

# Split the data into two groups
tsa_yes = df[df["tsaEligible"] == 1]["repairAmount"].dropna()
tsa_no = df[df["tsaEligible"] == 0]["repairAmount"].dropna()

# Function for confidence interval
def mean_ci(series, confidence=0.95):
    mean = series.mean()
    sd = series.std()
    n = len(series)
    sem = sd / np.sqrt(n)
    ci = stats.t.interval(confidence, n-1, loc=mean, scale=sem)
    return mean, ci

# Calculate CIs
mean_yes, ci_yes = mean_ci(tsa_yes)
mean_no, ci_no = mean_ci(tsa_no)

print("TSA Eligible mean repair:", mean_yes)
print("95% CI:", ci_yes)

print("\nTSA Not Eligible mean repair:", mean_no)
print("95% CI:", ci_no)

"""for Tsa eligible homes the mean repairamount was 5,916.06 with a 95% confidence interval of 5,885.19 to 5,946.93 and since the interval is very narrow that means the estimate is precise and for TSA non eligible homes the mean repair amount was 4,854.79 with a confidence interval of 4826.45 to 4883.13. the intervals do not overlap and it shows that eligible households do infact suffer higher repair related damages on average

Compare means between any two states TX vs LA
"""

state1 = df[df["damagedStateAbbreviation"] == "TX"]["repairAmount"].dropna()
state2 = df[df["damagedStateAbbreviation"] == "LA"]["repairAmount"].dropna()

mean1, ci1 = mean_ci(state1)
mean2, ci2 = mean_ci(state2)

print("TX mean + CI:", mean1, ci1)
print("LA mean + CI:", mean2, ci2)

"""Based off of the returned results Texas repair cost were significantly higher than lousiana's. Texas had a mean of 8,458 with a confidence interval between 8407 and 8508 while louisiana had a mean of 6765 and a confidence interval of 6711 to 6820 based off of these two states it is prevalent that texas repair amounts are substantially higher, so the region that you are in directly relates to the cost of repairs

Preform a two sample t-test
"""

t_stat, p_value = stats.ttest_ind(state1, state2, equal_var=False)  # Welch t-test
print("t-statistic:", t_stat)
print("p-value:", p_value)

"""the two sample t-test returned results that concluded that the results were highly significant showing a strong difference between the two groups. This confirms that TSA eligible households do in fact experience higher repair cost and is consistent with the programs gaols of assisting households with more severe disaster related damages and focusing

**Part 3 predictive modeling**

start by identifying the numberical and categorical values
"""

X = df[[
    "grossIncome",
    "repairAmount",
    "destroyed",
    "residenceType",
    "damagedStateAbbreviation"
]]
y = df["tsaEligible"].astype(int)

"""pre process and split"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# Define numerical and categorical features
numeric_features = ["grossIncome", "repairAmount", "destroyed"]
categorical_features = ["residenceType", "damagedStateAbbreviation"]

# Pipelines
numeric_transformer = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

categorical_transformer = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("encoder", OneHotEncoder(handle_unknown="ignore"))
])

# Full preprocessor
preprocessor = ColumnTransformer([
    ("num", numeric_transformer, numeric_features),
    ("cat", categorical_transformer, categorical_features)
])

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42
)

"""building teh pipeline from decision tree and random forest"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Decision Tree
dt_pipeline = Pipeline([
    ("preprocessor", preprocessor),
    ("classifier", DecisionTreeClassifier(max_depth=5, random_state=42))
])

# Random Forest
rf_pipeline = Pipeline([
    ("preprocessor", preprocessor),
    ("classifier", RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42, n_jobs=-1))
])

"""Decision tree model train"""

# Fit Decision Tree
dt_pipeline.fit(X_train, y_train)

"""Random forest model train"""

# Fit Random Forest
rf_pipeline.fit(X_train, y_train)

"""computing the metrics"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix

def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)
    return acc, prec, rec, cm

# Decision Tree metrics
dt_acc, dt_prec, dt_rec, dt_cm = evaluate_model(dt_pipeline, X_test, y_test)
print("Decision Tree Metrics:")
print(f"Accuracy: {dt_acc:.2f}")
print(f"Precision: {dt_prec:.2f}")
print(f"Recall: {dt_rec:.2f}")
print(f"Confusion Matrix:\n{dt_cm}")

# Random Forest metrics
rf_acc, rf_prec, rf_rec, rf_cm = evaluate_model(rf_pipeline, X_test, y_test)
print("\nRandom Forest Metrics:")
print(f"Accuracy: {rf_acc:.2f}")
print(f"Precision: {rf_prec:.2f}")
print(f"Recall: {rf_rec:.2f}")
print(f"Confusion Matrix:\n{rf_cm}")

"""analyzing the metrics
1. accuracy
decision tree: 0.77
radnom forest: 0.76
based off of the numbers both  models will be 76-77 percent accurate
2. precision
decision tree: 0.91
random forest: 0.92
the precision tells us that both models are very precise and can predict 90 percent of the time if someone is eligible for tsa
3. recall
decision tree: 0.44
random forest: 0.42
the model can only capture about 42-44 percent of the people who are actually eligible

based off of the numbers
decision tree is slightly higher recall and captures more eligible applicants but may overfit and not generalize well
random forest is more stable and less likely to overfit better overall reliability

random forest generalizes better but decision tree is more likely to capture more eligible applicants

random forest would be better because it is sligjtly mroe reliable with positive predictions and would avoid misallocating aid but both models have fairly low recall meaning many eligible applicants are missed

Stremlit portion
"""

!pip install streamlit plotly
import streamlit as st
import pandas as pd
import plotly.express as px

# --- Dashboard Title ---
st.title("FEMA Disaster Relief Dashboard")

# --- Load FEMA Dataset ---
df = pd.read_csv("IndividualAssistanceHousingRegistrantsLargeDisasters (2).csv")

# --- Data Preview ---
st.subheader("Data Preview")
st.write(df.head())

# --- Histogram of Repair Amount ---
st.subheader("Histogram of Repair Amount")
fig_hist = px.histogram(
    df,
    x="repairAmount",
    nbins=30,
    title="Distribution of Repair Amounts",
    labels={"repairAmount": "Repair Amount ($)"}
)
st.plotly_chart(fig_hist)

# --- Boxplot of Repair Amount by TSA Eligibility ---
st.subheader("Boxplot: Repair Amount by TSA Eligibility")
fig_box = px.box(
    df,
    x="tsaEligible",
    y="repairAmount",
    title="Repair Amount by TSA Eligibility",
    labels={
        "tsaEligible": "TSA Eligible (1 = Yes, 0 = No)",
        "repairAmount": "Repair Amount ($)"
    }
)
st.plotly_chart(fig_box)

# --- Insights ---
st.markdown(
    "*Insight:* The histogram shows the overall distribution of repair amounts across all households. "
    "The boxplot indicates that TSA-eligible households tend to have higher repair amounts, "
    "suggesting that the program is directed toward those with more severe property damage."
)